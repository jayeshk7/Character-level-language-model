{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import nltk\n",
    "import urllib.request\n",
    "import bs4\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10625\n",
      "3740\n"
     ]
    }
   ],
   "source": [
    "scrapped_data = urllib.request.urlopen('https://en.wikipedia.org/wiki/French_Revolution')\n",
    "article = scrapped_data.read()\n",
    "\n",
    "parsed_article = bs4.BeautifulSoup(article,'lxml')\n",
    "paragraphs = parsed_article.find_all(\"p\")\n",
    "\n",
    "articles = \"\"\n",
    "\n",
    "for p in paragraphs:\n",
    "    articles += p.text\n",
    "    \n",
    "train_data = articles.translate(str.maketrans('','',string.punctuation)).lower()\n",
    "train_data = train_data.translate(str.maketrans('','',string.digits))\n",
    "\n",
    "text = nltk.word_tokenize(train_data)\n",
    "stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "text = [words for words in text if words not in stop_words]\n",
    "print(len(text))\n",
    "\n",
    "vocab = set(text)\n",
    "print(len(vocab))\n",
    "vocab_size = len(set(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "window = 2\n",
    "ls = []\n",
    "\n",
    "for center_index in range(len(text)):\n",
    "    \n",
    "    for sub in range(max(0, center_index - window), center_index):\n",
    "        ls.append((center_index, sub))\n",
    "        \n",
    "    for add in range(center_index + 1, min(vocab_size - 1, center_index + window)+1):\n",
    "        ls.append((center_index, add))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2int = { word : i for i,word in enumerate(vocab) }\n",
    "int2word = { i : word for i,word in enumerate(vocab) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "class skipgram(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_size):\n",
    "        super(skipgram, self).__init__()\n",
    "        self.projection = torch.randn((vocab_size, embedding_size), requires_grad = True)\n",
    "        self.linear = nn.Linear(embedding_size, vocab_size)\n",
    "        \n",
    "    def forward(self, target_idx):\n",
    "        embedding = self.projection[target_idx]\n",
    "        prediction = self.linear(embedding.cuda())\n",
    "        \n",
    "        return prediction\n",
    "    \n",
    "net = skipgram(vocab_size, 150).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = list(net.parameters())\n",
    "params.extend(list(net.projection))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.003\n",
    "\n",
    "lossfn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(params, lr=alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.0872,  0.6562, -0.3686,  ..., -0.9246, -1.0376, -1.1621],\n",
      "        [ 1.3623,  1.2609, -0.3707,  ...,  0.1771, -2.2316,  2.1135],\n",
      "        [-1.1943,  1.1342,  1.0836,  ..., -0.4386,  0.2492, -1.3635],\n",
      "        ...,\n",
      "        [-0.9176, -1.0747, -0.1555,  ..., -0.4639, -2.2423, -1.6554],\n",
      "        [-1.1830,  0.6142,  0.1926,  ...,  0.1227,  1.4026,  0.3817],\n",
      "        [ 1.4625,  1.3568, -0.0613,  ...,  1.6573, -1.1089, -1.9093]],\n",
      "       requires_grad=True)\n",
      "tensor([[ 1.0872,  0.6562, -0.3686,  ..., -0.9246, -1.0376, -1.1621],\n",
      "        [ 1.3623,  1.2609, -0.3707,  ...,  0.1771, -2.2316,  2.1135],\n",
      "        [-1.1943,  1.1342,  1.0836,  ..., -0.4386,  0.2492, -1.3635],\n",
      "        ...,\n",
      "        [-0.9176, -1.0747, -0.1555,  ..., -0.4639, -2.2423, -1.6554],\n",
      "        [-1.1830,  0.6142,  0.1926,  ...,  0.1227,  1.4026,  0.3817],\n",
      "        [ 1.4625,  1.3568, -0.0613,  ...,  1.6573, -1.1089, -1.9093]],\n",
      "       requires_grad=True)\n",
      "tensor([[ 1.0872,  0.6562, -0.3686,  ..., -0.9246, -1.0376, -1.1621],\n",
      "        [ 1.3623,  1.2609, -0.3707,  ...,  0.1771, -2.2316,  2.1135],\n",
      "        [-1.1943,  1.1342,  1.0836,  ..., -0.4386,  0.2492, -1.3635],\n",
      "        ...,\n",
      "        [-0.9176, -1.0747, -0.1555,  ..., -0.4639, -2.2423, -1.6554],\n",
      "        [-1.1830,  0.6142,  0.1926,  ...,  0.1227,  1.4026,  0.3817],\n",
      "        [ 1.4625,  1.3568, -0.0613,  ...,  1.6573, -1.1089, -1.9093]],\n",
      "       requires_grad=True)\n",
      "tensor([[ 1.0872,  0.6562, -0.3686,  ..., -0.9246, -1.0376, -1.1621],\n",
      "        [ 1.3623,  1.2609, -0.3707,  ...,  0.1771, -2.2316,  2.1135],\n",
      "        [-1.1943,  1.1342,  1.0836,  ..., -0.4386,  0.2492, -1.3635],\n",
      "        ...,\n",
      "        [-0.9176, -1.0747, -0.1555,  ..., -0.4639, -2.2423, -1.6554],\n",
      "        [-1.1830,  0.6142,  0.1926,  ...,  0.1227,  1.4026,  0.3817],\n",
      "        [ 1.4625,  1.3568, -0.0613,  ...,  1.6573, -1.1089, -1.9093]],\n",
      "       requires_grad=True)\n",
      "tensor([[ 1.0872,  0.6562, -0.3686,  ..., -0.9246, -1.0376, -1.1621],\n",
      "        [ 1.3623,  1.2609, -0.3707,  ...,  0.1771, -2.2316,  2.1135],\n",
      "        [-1.1943,  1.1342,  1.0836,  ..., -0.4386,  0.2492, -1.3635],\n",
      "        ...,\n",
      "        [-0.9176, -1.0747, -0.1555,  ..., -0.4639, -2.2423, -1.6554],\n",
      "        [-1.1830,  0.6142,  0.1926,  ...,  0.1227,  1.4026,  0.3817],\n",
      "        [ 1.4625,  1.3568, -0.0613,  ...,  1.6573, -1.1089, -1.9093]],\n",
      "       requires_grad=True)\n",
      "tensor([[ 1.0872,  0.6562, -0.3686,  ..., -0.9246, -1.0376, -1.1621],\n",
      "        [ 1.3623,  1.2609, -0.3707,  ...,  0.1771, -2.2316,  2.1135],\n",
      "        [-1.1943,  1.1342,  1.0836,  ..., -0.4386,  0.2492, -1.3635],\n",
      "        ...,\n",
      "        [-0.9176, -1.0747, -0.1555,  ..., -0.4639, -2.2423, -1.6554],\n",
      "        [-1.1830,  0.6142,  0.1926,  ...,  0.1227,  1.4026,  0.3817],\n",
      "        [ 1.4625,  1.3568, -0.0613,  ...,  1.6573, -1.1089, -1.9093]],\n",
      "       requires_grad=True)\n",
      "tensor([[ 1.0872,  0.6562, -0.3686,  ..., -0.9246, -1.0376, -1.1621],\n",
      "        [ 1.3623,  1.2609, -0.3707,  ...,  0.1771, -2.2316,  2.1135],\n",
      "        [-1.1943,  1.1342,  1.0836,  ..., -0.4386,  0.2492, -1.3635],\n",
      "        ...,\n",
      "        [-0.9176, -1.0747, -0.1555,  ..., -0.4639, -2.2423, -1.6554],\n",
      "        [-1.1830,  0.6142,  0.1926,  ...,  0.1227,  1.4026,  0.3817],\n",
      "        [ 1.4625,  1.3568, -0.0613,  ...,  1.6573, -1.1089, -1.9093]],\n",
      "       requires_grad=True)\n",
      "tensor([[ 1.0872,  0.6562, -0.3686,  ..., -0.9246, -1.0376, -1.1621],\n",
      "        [ 1.3623,  1.2609, -0.3707,  ...,  0.1771, -2.2316,  2.1135],\n",
      "        [-1.1943,  1.1342,  1.0836,  ..., -0.4386,  0.2492, -1.3635],\n",
      "        ...,\n",
      "        [-0.9176, -1.0747, -0.1555,  ..., -0.4639, -2.2423, -1.6554],\n",
      "        [-1.1830,  0.6142,  0.1926,  ...,  0.1227,  1.4026,  0.3817],\n",
      "        [ 1.4625,  1.3568, -0.0613,  ...,  1.6573, -1.1089, -1.9093]],\n",
      "       requires_grad=True)\n",
      "tensor([[ 1.0872,  0.6562, -0.3686,  ..., -0.9246, -1.0376, -1.1621],\n",
      "        [ 1.3623,  1.2609, -0.3707,  ...,  0.1771, -2.2316,  2.1135],\n",
      "        [-1.1943,  1.1342,  1.0836,  ..., -0.4386,  0.2492, -1.3635],\n",
      "        ...,\n",
      "        [-0.9176, -1.0747, -0.1555,  ..., -0.4639, -2.2423, -1.6554],\n",
      "        [-1.1830,  0.6142,  0.1926,  ...,  0.1227,  1.4026,  0.3817],\n",
      "        [ 1.4625,  1.3568, -0.0613,  ...,  1.6573, -1.1089, -1.9093]],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "i = 1\n",
    "\n",
    "for iterations in range(epochs):\n",
    "    \n",
    "    for (center, context) in ls:\n",
    "        \n",
    "        context = word2int[text[context]]\n",
    "        center = word2int[text[center]]\n",
    "        \n",
    "        target = torch.tensor([context])\n",
    "        \n",
    "        score = net.forward(center).reshape(1,-1)\n",
    "        loss = lossfn(score.cuda(), target.cuda())\n",
    "        loss.requires_grad = True\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        print(net.projection)\n",
    "        \n",
    "        i += 1\n",
    "        if i%10 == 0:\n",
    "            break\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3742"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('0x7f395dd50a50', '0x7f3996b4c460')"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hex(id(next(net.parameters()))) , hex(id(net.projection))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.6820, -0.6981,  0.0846,  0.1088, -1.8055],\n",
      "        [-0.7347, -0.8329, -0.1547,  1.5112,  0.8998],\n",
      "        [ 1.0291, -1.5034, -0.9791,  0.4642, -0.6520]], requires_grad=True) torch.Size([3, 5])\n",
      "tensor([1, 1, 1]) torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "loss = nn.CrossEntropyLoss()\n",
    "inp = torch.randn(3, 5, requires_grad=True)\n",
    "print(inp, inp.shape)\n",
    "target = torch.empty(3, dtype=torch.long).random_(5)\n",
    "print(target, target.shape)\n",
    "output = loss(inp, target)\n",
    "output.requires_grad = True\n",
    "output.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
