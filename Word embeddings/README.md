# Word Embeddings

Word embeddings are an important part of Natural Language Processing. Having good dense representation of words really improves the performance on several tasks, also the representations capture various similarity and differences between words.

**DISCLAIMER** : Only 'Word embeddings.ipynb' of the above 3 codes is complete and works. I do not plan on completing the other 2 anytime soon, sorry! 

### Resources

Lectures : Stanford CS224n [Lecture 1 and 2](https://www.youtube.com/playlist?list=PLoROMvodv4rOhcuXMZkNm7j3fVwBBY42z)

Some important papers include :
1. [Original Word2Vec paper](https://arxiv.org/pdf/1301.3781.pdf)
2. [Negative Sampling paper](http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf)
3. [GloVe](https://nlp.stanford.edu/pubs/glove.pdf)

