{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy\n",
    "import nltk\n",
    "import re\n",
    "import string\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(r\"/home/kandpal/Desktop/Intro to NLP/Language Models/Dataset/shakespeare sonnets.txt\",\"r\",encoding='utf-8-sig')\n",
    "\n",
    "words=''\n",
    "for i in f:\n",
    "    words += i\n",
    "    \n",
    "# string mein store kiya kyuki string operations karne hai baad mein\n",
    "# utf-8-sig instead of just utf-8 kyuki kuch toh special characters read karne mein problem aata hai utf-8 ko"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = words.translate(str.maketrans('','',string.digits))\n",
    "words = words.translate(str.maketrans('','',string.punctuation))\n",
    "words = str(words.lower())\n",
    "\n",
    "words = nltk.word_tokenize(words)\n",
    "\n",
    "# digit punctuation nikalke tokenize kiya"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(set(words))\n",
    "vocab = set(words)\n",
    "\n",
    "int2word = {}\n",
    "word2int = {}\n",
    "\n",
    "for ids,unique in enumerate(vocab):\n",
    "    word2int[unique] = ids\n",
    "    int2word[ids] = unique\n",
    "    \n",
    "# yeh cell bhi self explanatory hai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "embbsize = 256\n",
    "# embbsize tera features ka dimensions hai\n",
    "\n",
    "vocab_vector = torch.zeros(1,vocab_size)\n",
    "# ek vector hai usme jo word rahega usko 1 kar dunga one hot ke liye, word represent karne ke liye\n",
    "\n",
    "embeddings = torch.randn(vocab_size, embbsize, requires_grad=True)\n",
    "# maine randomly initialised vector liya hai, requires grad true hai so uspe gradient update karunga har word ke liye"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self,embbsize, vocab_size, hidden_size=1024):\n",
    "        super(Model, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.lstm = nn.LSTM(embbsize, hidden_size, num_layers=2, dropout=0.5, batch_first=True)\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_size,vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output, hidden = self.lstm(x)    \n",
    "        out = self.fc(output)\n",
    "        return out,hidden\n",
    "    \n",
    "# simple model hai 2 stacked lstm, fir ek fc jo output dega"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Model(embbsize,vocab_size).cuda()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=1e-6, weight_decay=1e-4, amsgrad=True)\n",
    "lossfn = nn.CrossEntropyLoss() \n",
    "\n",
    "# self explanatory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "costs = []\n",
    "total_loss = []\n",
    "# batch_size = 56\n",
    "epoch = 20\n",
    "\n",
    "for i in range(epoch):\n",
    "    \n",
    "    coll_loss = 0\n",
    "    \n",
    "    for j,cor_word in enumerate(words):\n",
    "        \n",
    "        blue = vocab_vector\n",
    "        # vocab vector ka copy liya because har word ke liye ek one hot banana hai, har iteration mein\n",
    "        \n",
    "        blue[0][word2int[cor_word]] = 1\n",
    "        # jo word hai usko one kiya\n",
    "        \n",
    "        word_embed = torch.matmul(blue, embeddings)\n",
    "        # multiply kiya so that now mere paas jo word hai uska hi embeddings aa gaya\n",
    "        \n",
    "        word_embed = torch.unsqueeze(word_embed,0)\n",
    "        # lstm ko 1,1,inputsize chahiye hota hai na but sirf 1,inputsize hai abhi,,,, so unsqueeze\n",
    "        \n",
    "        if(j==len(words)-1):\n",
    "            ytarget = torch.tensor(0, dtype=torch.int64)\n",
    "            # yeh kuch bhi kiya hai iska solution nai samjha merko, jab last word ayega toh kya target rakhu\n",
    "            \n",
    "        else:\n",
    "            ytarget = torch.tensor(word2int[words[j+1]], dtype = torch.int64) #everythn should be a torch tensor in pytorch\n",
    "            # next word ka word2int kiya\n",
    "            \n",
    "        output,hidden1 = net(word_embed.cuda())   \n",
    "        \n",
    "        output = output.view(1,-1)   # shape is ([1,3119])\n",
    "        ytarget = torch.unsqueeze(ytarget,0)   # shape is ([1]) && value is ([1386])\n",
    "            \n",
    "        loss = lossfn(output.cuda(),ytarget.cuda())\n",
    "        costs.append(loss)\n",
    "        coll_loss += loss.item()\n",
    "            \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        loss.backward()\n",
    "        # isse mera embeddings ka bhi gradients compute hona chahiye, but mai confused hoon ki embedding matrix actually update hua ya nai\n",
    "            \n",
    "        optimizer.step()\n",
    "        # parameter update\n",
    "        \n",
    "    total_loss.append(coll_loss)\n",
    "        \n",
    "    print(\"epoch number\", i, coll_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(costs)\n",
    "plt.title('Costs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(total_loss)\n",
    "plt.title('Total loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage import gaussian_filter\n",
    "\n",
    "cost = [float(i.detach().cpu()) for i in costs]\n",
    "smooth = gaussian_filter(cost,800)\n",
    "plt.plot(smooth)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
