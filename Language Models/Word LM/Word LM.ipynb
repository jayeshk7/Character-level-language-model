{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport numpy\nimport nltk\nimport re\nimport string\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f = open(r\"/kaggle/input/shakes sonnets.txt\",\"r\",encoding='utf-8-sig')\n\nwords=''\nfor i in f:\n    words += i\n    \n# string mein store kiya kyuki string operations karne hai baad mein\n# utf-8-sig instead of just utf-8 kyuki kuch toh special characters read karne mein problem aata hai utf-8 ko","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"words = words.translate(str.maketrans('','',string.digits))\nwords = words.translate(str.maketrans('','',string.punctuation))\nwords = str(words.lower())\n\nwords = nltk.word_tokenize(words)\n\n# digit punctuation nikalke tokenize kiya","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vocab_size = len(set(words))\nvocab = set(words)\n\nint2word = {}\nword2int = {}\n\nfor ids,unique in enumerate(vocab):\n    word2int[unique] = ids\n    int2word[ids] = unique\n    \n# yeh cell bhi self explanatory hai","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embbsize = 256\n# embbsize tera features ka dimensions hai\n\nvocab_vector = torch.zeros(1,vocab_size)\n# ek vector hai usme jo word rahega usko 1 kar dunga one hot ke liye, word represent karne ke liye\n\nembeddings = torch.randn(vocab_size, embbsize, requires_grad=True)\n# maine randomly initialised vector liya hai, requires grad true hai so uspe gradient update karunga har word ke liye","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Model(nn.Module):\n    def __init__(self,embbsize, vocab_size, hidden_size=1024):\n        super(Model, self).__init__()\n        \n        self.hidden_size = hidden_size\n        \n        self.lstm = nn.LSTM(embbsize, hidden_size, num_layers=2, dropout=0.5, batch_first=True)\n        \n        self.fc = nn.Linear(hidden_size,vocab_size)\n\n    def forward(self, x):\n        output, hidden = self.lstm(x)    \n        out = self.fc(output)\n        return out,hidden\n    \n# simple model hai 2 stacked lstm, fir ek fc jo output dega","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"net = Model(embbsize,vocab_size).cuda()\noptimizer = torch.optim.Adam(net.parameters(), lr=1e-6, weight_decay=1e-4, amsgrad=True)\nlossfn = nn.CrossEntropyLoss() \n\n# self explanatory","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"costs = []\ntotal_loss = []\n# batch_size = 56\nepoch = 20\n\nfor i in range(epoch):\n    \n    coll_loss = 0\n    \n    for j,cor_word in enumerate(words):\n        \n        blue = vocab_vector\n        # vocab vector ka copy liya because har word ke liye ek one hot banana hai, har iteration mein\n        \n        blue[0][word2int[cor_word]] = 1\n        # jo word hai usko one kiya\n        \n        word_embed = torch.matmul(blue, embeddings)\n        # multiply kiya so that now mere paas jo word hai uska hi embeddings aa gaya\n        \n        word_embed = torch.unsqueeze(word_embed,0)\n        # lstm ko 1,1,inputsize chahiye hota hai na but sirf 1,inputsize hai abhi,,,, so unsqueeze\n        \n        if(j==len(words)-1):\n            ytarget = torch.tensor(0, dtype=torch.int64)\n            # yeh kuch bhi kiya hai iska solution nai samjha merko, jab last word ayega toh kya target rakhu\n            \n        else:\n            ytarget = torch.tensor(word2int[words[j+1]], dtype = torch.int64) #everythn should be a torch tensor in pytorch\n            # next word ka word2int kiya\n            \n        output,hidden1 = net(word_embed.cuda())   \n        \n        output = output.view(1,-1)   # shape is ([1,3119])\n        ytarget = torch.unsqueeze(ytarget,0)   # shape is ([1]) && value is ([1386])\n            \n        loss = lossfn(output.cuda(),ytarget.cuda())\n        costs.append(loss)\n        coll_loss += loss.item()\n            \n        optimizer.zero_grad()\n        \n        loss.backward()\n        # isse mera embeddings ka bhi gradients compute hona chahiye, but mai confused hoon ki embedding matrix actually update hua ya nai\n            \n        optimizer.step()\n        # parameter update\n        \n    total_loss.append(coll_loss)\n        \n    print(\"epoch number\", i, coll_loss)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(costs)\nplt.title('Costs')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(total_loss)\nplt.title('Total loss')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy.ndimage import gaussian_filter\n\ncost = [float(i.detach().cpu()) for i in costs]\nsmooth = gaussian_filter(cost,800)\nplt.plot(smooth)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}